{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mailbox\n",
    "import bs4\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "from datetime import datetime\n",
    "import dateparser\n",
    "\n",
    "import sqlite3\n",
    "import pickle\n",
    "\n",
    "\n",
    "from datasets import Dataset, load_dataset\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "import quopri\n",
    "import base64\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAILING_LISTS = [\n",
    "    \"pgsql-hackers\",\n",
    "    \"pgsql-performance\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    for mbox_path in sorted(Path(\"/home/kapi/Downloads/\").glob(\"pgsql-*\")):\n",
    "        mbox_name = mbox_path.name\n",
    "        with open(mbox_path, \"r\", encoding=\"utf-8\", errors=\"replace\") as f_read:\n",
    "            with open(Path(\".\") / mbox_name, \"w\", encoding=\"utf-8\") as f_write:\n",
    "                for line in f_read:\n",
    "                    line = line\n",
    "                    if line.startswith(\"From \"):\n",
    "                        if \"@\" not in line and not line.startswith(\"From bouncefilter\"):\n",
    "                            # The pgsql mailing lists are not mboxo-compliant.\n",
    "                            line = f\" {line}\"\n",
    "                    print(line, end=\"\", file=f_write)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stringify(m, depth=0):\n",
    "    if m.is_multipart():\n",
    "        parts = m.get_payload()\n",
    "        res = []\n",
    "        for part in parts:\n",
    "            res.append(stringify(part, depth=depth + 1))\n",
    "        return \"\\n\".join(res).strip()\n",
    "    content_type = m.get_content_type()\n",
    "    if content_type == \"text/plain\":\n",
    "        if \"Content-Disposition\" in m and \"attachment\" in str(m[\"Content-Disposition\"]).lower():\n",
    "            return \"\"\n",
    "        content_type_str = str(m[\"Content-Type\"])\n",
    "        if \"patch\" in content_type_str:\n",
    "            return \"\"\n",
    "        if \"name=\" in content_type_str:\n",
    "            return \"\"\n",
    "        charset = \"utf-8\"\n",
    "        if \"charset\" in content_type_str:\n",
    "            charset = content_type_str[content_type_str.find(\"charset=\") + len(\"charset=\"):].strip()\n",
    "            if \";\" in charset:\n",
    "                charset = charset[:charset.rfind(\";\")].strip()\n",
    "        content = m.get_payload(decode=True)\n",
    "        try:\n",
    "            content = content.decode(charset, errors=\"replace\")\n",
    "        except LookupError:\n",
    "            # e.g., UNKNOWN-8BIT, LookupError\n",
    "            content = content.decode(\"utf-8\", errors=\"replace\")\n",
    "        return content\n",
    "    elif content_type == \"text/html\":\n",
    "        if depth > 1:\n",
    "            # Skip - USUALLY duplicate of text/plain\n",
    "            return \"\"\n",
    "        content_type_str = str(m[\"Content-Type\"])\n",
    "        if \"patch\" in content_type_str:\n",
    "            return \"\"\n",
    "        if \"name=\" in content_type_str:\n",
    "            return \"\"\n",
    "        charset = \"utf-8\"\n",
    "        if \"charset\" in content_type_str:\n",
    "            charset = content_type_str[content_type_str.find(\"charset=\") + len(\"charset=\"):].strip()\n",
    "            if \";\" in charset:\n",
    "                charset = charset[:charset.rfind(\";\")].strip()\n",
    "        content = m.get_payload(decode=True)\n",
    "        try:\n",
    "            content = content.decode(charset, errors=\"replace\")\n",
    "        except LookupError:\n",
    "            # e.g., UNKNOWN-8BIT, LookupError\n",
    "            content = content.decode(\"utf-8\", errors=\"replace\")\n",
    "        soup = bs4.BeautifulSoup(content, 'lxml')\n",
    "        return soup.get_text()\n",
    "    else:\n",
    "        # Skip - images etc\n",
    "        return \"\"\n",
    "    \n",
    "class UnionFind:\n",
    "    def __init__(self):\n",
    "        self.parent = {}\n",
    "        self.size = {}\n",
    "\n",
    "    def __contains__(self, x):\n",
    "        return x in self.parent\n",
    "\n",
    "    def add(self, x):\n",
    "        if x not in self.parent:\n",
    "            self.parent[x] = x\n",
    "            self.size[x] = 1\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def find(self, x):\n",
    "        if self.parent[x] == x:\n",
    "            return x\n",
    "        self.parent[x] = self.find(self.parent[x])\n",
    "        return self.parent[x]\n",
    "        \n",
    "    def union(self, a, b):\n",
    "        a = self.find(a)\n",
    "        b = self.find(b)\n",
    "        if a != b:\n",
    "            if self.size[a] < self.size[b]:\n",
    "                a, b = b, a\n",
    "            self.parent[b] = a\n",
    "            self.size[a] += self.size[b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "MBOXES = {}\n",
    "for mailing_list in MAILING_LISTS:\n",
    "    MBOXES[mailing_list] = {}\n",
    "    for mbox_path in sorted(Path(\"./data\").glob(f\"{mailing_list}*\")):\n",
    "        mbox_name = mbox_path.name\n",
    "        MBOXES[mailing_list][mbox_name] = mailbox.mbox(mbox_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for msg_i, msg in enumerate(MBOXES[\"pgsql-hackers\"][\"pgsql-hackers.202401\"]):\n",
    "#     if msg[\"Subject\"] == \"pg_stat_advisor extension\":\n",
    "#         print(msg[\"Subject\"], msg_i)\n",
    "#         break\n",
    "\n",
    "\n",
    "# msg = MBOXES[\"pgsql-hackers\"][\"pgsql-hackers.202401\"][2711]\n",
    "# # for x in msg.get_payload():\n",
    "# #     print(x[\"Content-Type\"])\n",
    "# #     print(x.get_payload()[:500])\n",
    "\n",
    "# msg_contents = stringify(msg)\n",
    "# print(msg_contents[:50000])\n",
    "    \n",
    "# # msg_ts_str = msg[\"Date\"].strip()\n",
    "# # # There are a bazillion ways that dates appear on the mailing list.\n",
    "# # if msg_ts_str.endswith(\")\"):\n",
    "# #     # (%Z) doesn't work for \"AST\" in \"Sun, 4 Jan 1998 03:26:34 -0400 (AST)\"\n",
    "# #     # fmt = \"%a, %d %b %Y %H:%M:%S %z (%Z)\"\n",
    "# #     msg_ts_str = msg_ts_str[:msg_ts_str.rfind(\"(\")].strip()\n",
    "# # if \",\" in msg_ts_str:\n",
    "# #     # And at some point they dropped the \"%a,\"\n",
    "# #     msg_ts_str = msg_ts_str[msg_ts_str.find(\",\")+1:].strip()\n",
    "# # # And there are some completely wack dates like \"28 Feb 1998 22:25:42 -7700\"\n",
    "# # msg_ts = dateparser.parse(msg_ts_str)\n",
    "# # if msg_ts is None:\n",
    "# #     print(msg_ts_str, msg)\n",
    "# # msg_ts = msg_ts.isoformat(\" \")\n",
    "# # # This is stupid. We'll use an offset instead.\n",
    "# # cur.execute(\"INSERT INTO email VALUES (?, ?, ?, ?)\", (msg_id, msg_contents, mbox_name, msg_i))\n",
    "\n",
    "# threads = {}\n",
    "# for m in mbox:\n",
    "#     key = m[\"Message-ID\"].strip()\n",
    "#     rep = uf.find(key)\n",
    "#     thread_rep = mbox[mbox_map[key]]\n",
    "#     msg = mbox[mbox_map[key]]\n",
    "#     if rep not in threads:\n",
    "#         threads[rep] = []\n",
    "#     threads[rep].append(msg[\"Subject\"])\n",
    "\n",
    "# threads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<AANLkTilvwQXHVkeAMmAlAnYYNEMRsTZG7pmzZxoyMRBz@mail.gmail.com>', '<asdasd>']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_refs(s):\n",
    "    def get_mime(s):\n",
    "        encoded_word_regex = r'=\\?{1}(.+)\\?{1}([B|Q])\\?{1}(.+)\\?{1}='\n",
    "        charset, encoding, encoded_text = re.match(encoded_word_regex, s).groups()\n",
    "        if encoding == 'B':\n",
    "            byte_string = base64.b64decode(encoded_text)\n",
    "        elif encoding == 'Q':\n",
    "            byte_string = quopri.decodestring(encoded_text)\n",
    "        return byte_string.decode(charset)\n",
    "\n",
    "    s_old = s\n",
    "    s = s.strip()\n",
    "    if s.startswith(\"=?\"):\n",
    "        tmp = []\n",
    "        for line in s.split(\"\\n\"):\n",
    "            line = line.strip()\n",
    "            tmp.append(get_mime(line))\n",
    "        s = \"\".join(tmp)\n",
    "\n",
    "    refs = []\n",
    "    while \"<\" in s:\n",
    "        s = s[s.find(\"<\"):]\n",
    "        ref = s[:s.find(\">\") + 1]\n",
    "        # Some emails will just have two @'s, so you can't check that.\n",
    "        # e.g., <20000717201117.AAA24984@mailserver.sixdegrees.com@sorrow>\n",
    "        # And you're not guaranteed complete references either, so only save the good ones.\n",
    "        if ref.startswith(\"<\") and ref.endswith(\">\"):\n",
    "            refs.append(ref)\n",
    "        if \">\" not in s:\n",
    "            break\n",
    "        s = s[s.find(\">\") + 1:]\n",
    "    return refs\n",
    "\n",
    "\n",
    "\n",
    "blah = \"\"\"=?utf-8?Q?_<AANLkTikaQj-YKgYRTWg5UKOZ4uIO13q8gOZRevOnR6Iz@mail.gma?=\n",
    "\t=?utf-8?Q?il.com>_<AANLkTikFqI88TQDI359=5FpWgOa98RAApHaC3rj9ZNB-4V@mail.gmai?=\n",
    "\t=?utf-8?Q?l.com>_<20100524160843.5158c3d5.wmoran@potentialtech.com>_<AAN?=\n",
    "\t=?utf-8?Q?LkTilvwQXHVkeAMmAlAnYYNEMRsTZG7pmzZxoyMRBz@mail.gmail.com>?=\"\"\"\n",
    "blah2 = \"\"\"<80c80b4a-b87b-456f-bd46-1ae326601d79.xiyuan.zr@alibaba-inc.com>\"\"\"\n",
    "\n",
    "blah3 = \"\"\" =?utf-8?Q?_<AANLkTikaQj-YKgYRTWg5UKOZ4uIO13q8gOZRevOnR6Iz@mail.gma?=\n",
    "\t=?utf-8?Q?il.com>_<AANLkTikFqI88TQDI359=5FpWgOa98RAApHaC3rj9ZNB-4V@mail.gmai?=\n",
    "\t=?utf-8?Q?l.com>_<20100524160843.5158c3d5.wmoran@potentialtech.com>_<AAN?=\n",
    "\t=?utf-8?Q?LkTilvwQXHVkeAMmAlAnYYNEMRsTZG7pmzZxoyMRBz@mail.gmail.com>?=\"\"\"\n",
    "blah4 =\"\"\"<AANLkTilvwQXHVkeAMmAlAnYYNEMRsTZG7pmzZxoyMRBz@mail.gmail.com><asdasd>\"\"\"\n",
    "\n",
    "get_refs(blah4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note that num_known_messages may not equal num_available_messages because some emails are not available on the mailing list.\n",
      "pgsql-hackers: num_known_messages=588636\n",
      "pgsql-performance: num_known_messages=65043\n"
     ]
    }
   ],
   "source": [
    "UF = {}\n",
    "\n",
    "for mailing_list, mboxes in MBOXES.items():\n",
    "    uf_path = Path(f\"uf_{mailing_list}.pickle\")\n",
    "    if not uf_path.exists():\n",
    "        uf = UnionFind()\n",
    "        with tqdm(total=len(mboxes)) as pbar:\n",
    "            for mbox_name, mbox in mboxes.items():\n",
    "                pbar.set_description(f\"Processing UF {mbox_name}\")\n",
    "                for msg_i, msg in enumerate(mbox):\n",
    "                    if msg[\"Message-Id\"] is None:\n",
    "                        continue\n",
    "                    msg_id = msg[\"Message-ID\"].strip()\n",
    "                    uf.add(msg_id)\n",
    "                    for ref_key in [\"References\", \"In-Reply-To\"]:\n",
    "                        if ref_key not in msg:\n",
    "                            continue\n",
    "                        ref_cands = str(msg[ref_key])\n",
    "                        for ref_id in get_refs(ref_cands):\n",
    "                            uf.add(ref_id)\n",
    "                            uf.union(msg_id, ref_id)\n",
    "                pbar.update(1)\n",
    "        with open(f\"uf_{mailing_list}.pickle\", \"wb\") as f:\n",
    "            pickle.dump(uf, f)\n",
    "    with open(f\"uf_{mailing_list}.pickle\", \"rb\") as f:\n",
    "        UF[mailing_list] = pickle.load(f)\n",
    "\n",
    "print(\"Note that num_known_messages may not equal num_available_messages because some emails are not available on the mailing list.\")\n",
    "for key in UF:\n",
    "    num_known_messages = len(UF[key].parent)\n",
    "    print(f\"{key}: {num_known_messages=}\")            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pgsql-hackers: num_emails=579909 num_threads=58504\n",
      "pgsql-performance: num_emails=63689 num_threads=9718\n"
     ]
    }
   ],
   "source": [
    "EMAILS = {}\n",
    "THREADS = {}\n",
    "\n",
    "for mailing_list, mboxes in MBOXES.items():\n",
    "    threads_path = Path(f\"threads_{mailing_list}.pickle\")\n",
    "    if not threads_path.exists():\n",
    "        emails = {}\n",
    "        threads = {}\n",
    "        uf = UF[mailing_list]\n",
    "        max_messages = len(uf.parent)\n",
    "        with tqdm(total=max_messages) as pbar:\n",
    "            for mbox_name, mbox in mboxes.items():\n",
    "                pbar.set_description(f\"Building threads out of messages from: {mbox_name}\")\n",
    "                for msg_i, msg in enumerate(mbox):\n",
    "                    pbar.update(1)\n",
    "                    if msg[\"Message-Id\"] is None:\n",
    "                        continue\n",
    "                    msg_id = msg[\"Message-ID\"].strip()\n",
    "                    assert msg_id not in emails\n",
    "                    emails[msg_id] = msg\n",
    "\n",
    "                    thread_rep = uf.find(msg_id)\n",
    "                    thread_key = msg_id\n",
    "                    if thread_rep not in threads:\n",
    "                        threads[thread_rep] = [thread_key]\n",
    "                    else:\n",
    "                        threads[thread_rep].append(thread_key)\n",
    "        with open(f\"emails_{mailing_list}.pickle\", \"wb\") as f:\n",
    "            pickle.dump(emails, f)\n",
    "        with open(f\"threads_{mailing_list}.pickle\", \"wb\") as f:\n",
    "            pickle.dump(threads, f)\n",
    "    with open(f\"emails_{mailing_list}.pickle\", \"rb\") as f:\n",
    "        EMAILS[mailing_list] = pickle.load(f)\n",
    "    with open(f\"threads_{mailing_list}.pickle\", \"rb\") as f:\n",
    "        THREADS[mailing_list] = pickle.load(f)\n",
    "    \n",
    "for mailing_list in MAILING_LISTS:\n",
    "    num_emails = len(EMAILS[mailing_list])\n",
    "    num_threads = len(THREADS[mailing_list])\n",
    "    print(f\"{mailing_list}: {num_emails=} {num_threads=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = {}\n",
    "\n",
    "for mailing_list in MAILING_LISTS:\n",
    "    dataset = []\n",
    "    dataset_path = Path(f\"dataset_{mailing_list}.pickle\")\n",
    "    if not dataset_path.exists():\n",
    "        emails = EMAILS[mailing_list]\n",
    "        threads = THREADS[mailing_list]\n",
    "        num_threads = len(THREADS[mailing_list])\n",
    "        with tqdm(total=num_threads) as pbar:\n",
    "            pbar.set_description(f\"Processing threads for {mailing_list}\")\n",
    "            for thread_rep, msg_ids in threads.items():\n",
    "                pbar.update(1)\n",
    "                if len(msg_ids) == 0:\n",
    "                    continue\n",
    "\n",
    "                excessively_long_emails = [\n",
    "                    # pgsql-hackers\n",
    "                    # https://www.postgresql.org/message-id/Pine.UW2.4.21.0110281928170.2268-100000%40server.pyrenet.fr\n",
    "                    \"<Pine.UW2.4.21.0110281928170.2268-100000@server.pyrenet.fr>\",\n",
    "                    # https://www.postgresql.org/message-id/AANLkTimU421rHhBhurUMHN9UhJbh38k657vWMsXNs5i-%40mail.gmail.com\n",
    "                    \"<AANLkTimU421rHhBhurUMHN9UhJbh38k657vWMsXNs5i-@mail.gmail.com>\",\n",
    "                    # https://www.postgresql.org/message-id/20191105024819.GB12780%40momjian.us\n",
    "                    \"<20191105024819.GB12780@momjian.us>\",\n",
    "                    # pgsql-performance\n",
    "                    # https://www.postgresql.org/message-id/CAN0SRDFgPhyyP2yBphfVEYO0-K2uFXUk%2BC6euTCdx9jt5QkYwQ%40mail.gmail.com\n",
    "                    \"<CAN0SRDFgPhyyP2yBphfVEYO0-K2uFXUk+C6euTCdx9jt5QkYwQ@mail.gmail.com>\",\n",
    "                    # https://www.postgresql.org/message-id/0C3B1B2B-6571-477D-A89C-6FA7B6259840%40spectralogic.com\n",
    "                    \"<0C3B1B2B-6571-477D-A89C-6FA7B6259840@spectralogic.com>\",\n",
    "                    # https://www.postgresql.org/message-id/CAAUL%3DcFcvUo%3D7b4T-K5PqiqrF6etp59qcgv77DyK2Swa4VhYuQ%40mail.gmail.com\n",
    "                    \"<CAAUL=cFcvUo=7b4T-K5PqiqrF6etp59qcgv77DyK2Swa4VhYuQ@mail.gmail.com>\",\n",
    "                ]\n",
    "                if any(x_id in msg_ids for x_id in excessively_long_emails):\n",
    "                    continue\n",
    "                if any(str(emails[msg_id][\"Subject\"]).lower() == \"unsubscribe\" for msg_id in msg_ids):\n",
    "                    # Signatures.\n",
    "                    continue\n",
    "\n",
    "                all_empty = True\n",
    "                data_row = []\n",
    "                msg_op = emails[msg_ids[0]][\"From\"]\n",
    "                for msg_i, msg_id in enumerate(msg_ids):\n",
    "                    msg = emails[msg_id]\n",
    "                    msg_date = str(msg[\"Date\"])\n",
    "                    msg_subject = str(msg[\"Subject\"])\n",
    "                    msg_from = str(msg[\"From\"])\n",
    "                    try:\n",
    "                        msg_contents = stringify(msg)\n",
    "                    except Exception as e:\n",
    "                        print(msg)\n",
    "                        raise e\n",
    "                    if len(msg_contents) > 0:\n",
    "                        all_empty = False\n",
    "\n",
    "                    msg_from_op = msg_from == msg_op\n",
    "\n",
    "                    msg_dict = {\n",
    "                        \"msg_date\": msg_date,\n",
    "                        \"msg_subject\": msg_subject,\n",
    "                        \"msg_from\": msg_from,\n",
    "                        \"msg_contents\": msg_contents,\n",
    "                        \"msg_from_op\": msg_from_op,\n",
    "                    }\n",
    "                    data_row.append(msg_dict)\n",
    "                if not all_empty:\n",
    "                    dataset.append(data_row)\n",
    "        with open(dataset_path, \"wb\") as f:\n",
    "            pickle.dump(dataset, f)\n",
    "    with open(dataset_path, \"rb\") as f:\n",
    "        DATASET[mailing_list] = {}\n",
    "        DATASET[mailing_list][\"threads\"] = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKENIZER_MODEL = \"unsloth/Llama-3.2-3B-bnb-4bit\"\n",
    "MAX_SEQ_LEN = 2048\n",
    "\n",
    "SYSTEM_PROMPT = \"\"\"You are an expert PostgreSQL developer helping people on the '{}' mailing list.\n",
    "You are composing a reply to resolve the email chain below.\n",
    "\n",
    "### Original Email:\n",
    "Date: {}\n",
    "Subject: {}\n",
    "Contents: {}\n",
    "\n",
    "### Some emails may be omitted for brevity.\"\"\"\n",
    "\n",
    "USER_PROMPT = \"\"\"### Latest Email:\n",
    "Date: {}\n",
    "Subject: {}\n",
    "Contents: {}\n",
    "\"\"\"\n",
    "\n",
    "GPT_PROMPT = \"\"\"### Your Response:\n",
    "Date: {}\n",
    "Subject: {}\n",
    "Contents: {}\"\"\"\n",
    "\n",
    "# unsloth\n",
    "# {'conversations': [\n",
    "#   {'from': 'human', 'value': 'Give three tips for staying healthy.'},\n",
    "#   {'from': 'gpt', 'value': '1. Eat a balanced and nutritious ...'},\n",
    "#   {'from': 'human', 'value': 'Describe what a monotheistic religion is.'},\n",
    "#   {'from': 'gpt', 'value': 'A monotheistic religion is a type of ...'}\n",
    "# ]}\n",
    "CONVERSATIONS = {}\n",
    "\n",
    "for mailing_list in MAILING_LISTS:\n",
    "    conversations = []\n",
    "    conversations_path = Path(f\"conversations_{mailing_list}.pickle\")\n",
    "    if not conversations_path.exists():\n",
    "        threads = DATASET[mailing_list][\"threads\"]\n",
    "        num_threads = len(threads)\n",
    "        with tqdm(total=num_threads) as pbar:\n",
    "            pbar.set_description(f\"Processing threads for {mailing_list}\")\n",
    "            for thread in threads:\n",
    "                pbar.update(1)\n",
    "                num_emails = len(thread)\n",
    "                if num_emails <= 1:\n",
    "                    # Zetsubo.\n",
    "                    continue\n",
    "\n",
    "                first_email = thread[0]\n",
    "\n",
    "                for email_idx in range(1, num_emails):\n",
    "                    # To make tokenizing possible.\n",
    "                    conversation = []\n",
    "                    conversation.append({\n",
    "                        \"from\": \"system\",\n",
    "                        \"value\": SYSTEM_PROMPT.format(\n",
    "                            mailing_list,\n",
    "                            first_email[\"msg_date\"],\n",
    "                            first_email[\"msg_subject\"],\n",
    "                            first_email[\"msg_contents\"],\n",
    "                        ),\n",
    "                    })\n",
    "                    prev_email = thread[email_idx - 1]\n",
    "                    conversation.append({\n",
    "                        \"from\": \"user\",\n",
    "                        \"value\": USER_PROMPT.format(\n",
    "                            prev_email[\"msg_date\"],\n",
    "                            prev_email[\"msg_subject\"],\n",
    "                            prev_email[\"msg_contents\"],\n",
    "                        )\n",
    "                    })\n",
    "\n",
    "                    latest_email = thread[email_idx]\n",
    "                    conversation.append({\n",
    "                        \"from\": \"assistant\",\n",
    "                        \"value\": GPT_PROMPT.format(\n",
    "                            latest_email[\"msg_date\"],\n",
    "                            latest_email[\"msg_subject\"],\n",
    "                            latest_email[\"msg_contents\"],\n",
    "                        )\n",
    "                    })\n",
    "                    conversations.append(conversation)\n",
    "        with open(conversations_path, \"wb\") as f:\n",
    "            pickle.dump(conversations, f)\n",
    "    with open(conversations_path, \"rb\") as f:\n",
    "        CONVERSATIONS[mailing_list] = {}\n",
    "        CONVERSATIONS[mailing_list][\"conversations\"] = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKENIZER_MODEL = \"unsloth/Llama-3.2-3B-bnb-4bit\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Forming prompts for pgsql-hackers:   0%|          | 1978/520899 [00:08<30:11, 286.49it/s]"
     ]
    }
   ],
   "source": [
    "TEXTS = {}\n",
    "for mailing_list in MAILING_LISTS:\n",
    "    conversations = CONVERSATIONS[mailing_list][\"conversations\"]\n",
    "    texts_path = Path(f\"texts_{mailing_list}.pickle\")\n",
    "    untokenized_texts_path = Path(f\"untokenized_texts_{mailing_list}.pickle\")\n",
    "    if not texts_path.exists():\n",
    "        texts = []\n",
    "        untokenized_texts = []\n",
    "        num_conversations = len(conversations)\n",
    "        with tqdm(total=num_conversations) as pbar:\n",
    "            pbar.set_description(f\"Forming prompts for {mailing_list}\")\n",
    "            for conversation in conversations:\n",
    "                pbar.update(1)\n",
    "                prompt = []\n",
    "                prompt.append(tokenizer.bos_token)\n",
    "                for msg_idx, msg in enumerate(conversation):\n",
    "                    if msg_idx == 0:\n",
    "                        assert msg[\"from\"] == \"system\"\n",
    "                    prompt.append(r\"\"\"<|start_header_id|>\"\"\")\n",
    "                    prompt.append(msg[\"from\"])\n",
    "                    prompt.append(r\"\"\"<|end_header_id|>\"\"\" + \"\\n\\n\")\n",
    "                    prompt.append(msg[\"value\"])\n",
    "                    prompt.append(r\"\"\"<|eot_id|>\"\"\")\n",
    "                prompt.append(tokenizer.eos_token)\n",
    "                text = \"\".join(prompt)\n",
    "                if len(text) > 7500:\n",
    "                    untokenized_texts.append(text)\n",
    "                    continue\n",
    "                toklen = len(tokenizer.tokenize(text))\n",
    "                texts.append((text, toklen))\n",
    "        with open(untokenized_texts_path, \"wb\") as f:\n",
    "            pickle.dump(untokenized_texts, f)\n",
    "        with open(texts_path, \"wb\") as f:\n",
    "            pickle.dump(texts, f)\n",
    "    with open(texts_path, \"rb\") as f:\n",
    "        TEXTS[mailing_list] = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pgsql-hackers 1562697\n",
      "pgsql-performance 161640\n"
     ]
    }
   ],
   "source": [
    "FILTERED_TEXTS = {}\n",
    "for mailing_list in MAILING_LISTS:\n",
    "    num_emails = 0\n",
    "    conversations = CONVERSATIONS[mailing_list][\"conversations\"]\n",
    "    for conversation in conversations:\n",
    "        num_emails += len(conversation)\n",
    "    print(mailing_list, num_emails)\n",
    "    filtered_texts_path = Path(f\"filteredtexts_{mailing_list}.pickle\")\n",
    "    if not filtered_texts_path.exists():\n",
    "        filtered_texts = {\"text\": [text for (text, toklen) in TEXTS[mailing_list] if toklen <= 2048]}\n",
    "        with open(filtered_texts_path, \"wb\") as f:\n",
    "            pickle.dump(filtered_texts, f)\n",
    "    with open(filtered_texts_path, \"rb\") as f:\n",
    "        FILTERED_TEXTS[mailing_list] = {}\n",
    "        FILTERED_TEXTS[mailing_list] = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pgsql-hackers Dataset({\n",
      "    features: ['threads'],\n",
      "    num_rows: 58371\n",
      "})\n",
      "pgsql-hackers-processed Dataset({\n",
      "    features: ['text'],\n",
      "    num_rows: 384601\n",
      "})\n",
      "pgsql-performance Dataset({\n",
      "    features: ['threads'],\n",
      "    num_rows: 9686\n",
      "})\n",
      "pgsql-performance-processed Dataset({\n",
      "    features: ['text'],\n",
      "    num_rows: 33722\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "for mailing_list in MAILING_LISTS:\n",
    "    for dsname in [mailing_list, f\"{mailing_list}-processed\"]:\n",
    "        ds = load_dataset(f\"wanshenl/{dsname}\", split=\"train\")\n",
    "        print(dsname, ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pgsql-performance #char\n",
      "count           9718\n",
      "mean         13717.8\n",
      "std          38778.5\n",
      "min               90\n",
      "50%           6600.5\n",
      "75%            14935\n",
      "80%          18213.8\n",
      "85%          22528.1\n",
      "90%          29552.5\n",
      "95%          44991.1\n",
      "99%           102610\n",
      "max      2.34708e+06\n",
      "dtype: object\n",
      "pgsql-performance #space\n",
      "count       9718\n",
      "mean      2507.9\n",
      "std      7237.06\n",
      "min           11\n",
      "50%         1101\n",
      "75%       2631.5\n",
      "80%       3207.2\n",
      "85%       4023.9\n",
      "90%       5383.9\n",
      "95%      8526.05\n",
      "99%      20354.7\n",
      "max       469102\n",
      "dtype: object\n",
      "pgsql-hackers #char\n",
      "count          57080\n",
      "mean         21945.5\n",
      "std           120435\n",
      "min               88\n",
      "50%           5244.5\n",
      "75%          15143.2\n",
      "80%          19823.8\n",
      "85%          27065.3\n",
      "90%          40414.6\n",
      "95%          74222.4\n",
      "99%           260732\n",
      "max      9.91286e+06\n",
      "dtype: object\n",
      "pgsql-hackers #space\n",
      "count          57080\n",
      "mean         3479.23\n",
      "std          20244.1\n",
      "min               11\n",
      "50%              820\n",
      "75%             2403\n",
      "80%             3158\n",
      "85%             4314\n",
      "90%           6472.1\n",
      "95%            11854\n",
      "99%          40696.9\n",
      "max      1.70126e+06\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "print(\"pgsql-performance #char\")\n",
    "s = pd.Series([sum([len(x[\"value\"]) for x in ds[i][\"conversations\"]]) for i in range(len(ds))])\n",
    "print(s.describe([.75, .8, .85, .9, .95, .99]).apply(lambda x: format(x, 'g')))\n",
    "print(\"pgsql-performance #space\")\n",
    "s = pd.Series([sum([x[\"value\"].count(\" \") for x in ds[i][\"conversations\"]]) for i in range(len(ds))])\n",
    "print(s.describe([.75, .8, .85, .9, .95, .99]).apply(lambda x: format(x, 'g')))\n",
    "print(\"pgsql-hackers #char\")\n",
    "s = pd.Series([sum([len(x[\"value\"]) for x in ds2[i][\"conversations\"]]) for i in range(len(ds2))])\n",
    "print(s.describe([.75, .8, .85, .9, .95, .99]).apply(lambda x: format(x, 'g')))\n",
    "print(\"pgsql-hackers #space\")\n",
    "s = pd.Series([sum([x[\"value\"].count(\" \") for x in ds2[i][\"conversations\"]]) for i in range(len(ds2))])\n",
    "print(s.describe([.75, .8, .85, .9, .95, .99]).apply(lambda x: format(x, 'g')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"unsloth/Llama-3.2-3B-bnb-4bit\")\n",
    "# tokenizer.tokenize(\"select cat from foo as xxx where xxx.value = 'joe'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pgsql-hackers #tokens\n",
      "count          57080\n",
      "mean          6109.7\n",
      "std          36969.3\n",
      "min               34\n",
      "50%             1418\n",
      "75%          4079.25\n",
      "80%           5378.2\n",
      "85%          7300.15\n",
      "90%            10907\n",
      "95%          20001.9\n",
      "99%          71109.7\n",
      "max      2.92211e+06\n",
      "dtype: object\n",
      "pgsql-performance #tokens\n",
      "count       9718\n",
      "mean     3847.36\n",
      "std      14574.2\n",
      "min           33\n",
      "50%       1780.5\n",
      "75%      4079.25\n",
      "80%       4990.2\n",
      "85%      6201.45\n",
      "90%       8167.9\n",
      "95%      12338.1\n",
      "99%        28180\n",
      "max       960927\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# for mailing_list in MAILING_LISTS:\n",
    "#     toklen_path = Path(f\"toklen_{mailing_list}.pickle\")\n",
    "#     if not toklen_path.exists():\n",
    "#         s = []\n",
    "#         ds = load_dataset(f\"wanshenl/{mailing_list}\", split=\"train\")\n",
    "#         for i in tqdm(range(len(ds))):\n",
    "#             messages = ds[i][\"conversations\"]\n",
    "#             toklens = [len(tokenizer.tokenize(msg[\"value\"])) for msg in messages]\n",
    "#             toklen = sum(toklens)\n",
    "#             s.append(toklen)\n",
    "#         s = pd.Series(s)\n",
    "#         with open(toklen_path, \"wb\") as f:\n",
    "#             pickle.dump(s, f)\n",
    "#     with open(toklen_path, \"rb\") as f:\n",
    "#         s = pickle.load(f)\n",
    "#     print(f\"{mailing_list} #tokens\")\n",
    "#     print(s.describe([.75, .8, .85, .9, .95, .99]).apply(lambda x: format(x, 'g')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16384"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ds = load_dataset(f\"wanshenl/pgsql-hackers\", split=\"train\")\n",
    "# with open(Path(f\"toklen_pgsql-hackers.pickle\"), \"rb\") as f:\n",
    "#     s = pickle.load(f)\n",
    "# ds[\"conversations\"][s.argmax()]\n",
    "16*1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         369\n",
       "1          48\n",
       "2         134\n",
       "3        3408\n",
       "4         588\n",
       "         ... \n",
       "57075    1721\n",
       "57076     508\n",
       "57077     490\n",
       "57078     365\n",
       "57079     347\n",
       "Length: 53454, dtype: int64"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s[s <= 16384]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
